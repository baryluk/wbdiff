#!/usr/bin/env python3

# wbdiff - sophisticated diff utility
#
# Copyright: Witold Baryluk, 2013, 2021
#
# MIT license
#
# This is simplistic tool for detecting moved blocks
# and edits of code in text files.
#
# It can be used just like normal diff utility, but it additionally
# supports line movement detection, fuzzy matching, colored output,
# collapsing very long runs of similar lines / moves
# and runs up much slower.
#
# Algorithm is O(k⋅n²⋅l²), where k is context lenght, n number
# of lines, l lenght of lines.
#
# If no fuzzy matching is used, then algorithm is basically O(n²⋅l).
#
# It could be improved in the future tho, to be O(n⋅l) probably tho.
#
# There are multiple features on the horizon, like automatic compressed
# files support, adaptative context length estimation, white space removal,
# custom replacments matching,
# speed optimizations for case of small number of block movements,
# side-by-side comparission (console and HTML)
# and graphical visualizer (using GraphViz probably) and GUI frontend.
#
# If you want to pipe output to `less`, use `less -RS`
#
# This script runs in 0.165 seconds on my benchmark (300 lines),
# compared to 0.019 seconds by `diff -U 300` ("unified diff", with big context).
#
# Difference is big, but this program should be usable for manual execution
# with files of about 5000 lines. For patches, and automatic execution
# classic diff algorithm is probably better because of O(n) algorithm used.
#
# Similar good programs, which can align codes in non-sequential manner:
#  WinDiff by Microsoft (free, comes with source code, runs under Windows, and WINE)
#    - very good results, but old UI
#  WinMerge (free open-source, for Windows, runs also on WINE, native port in preparation
#    - modern UI, but results are inferior
#  Araxis Merge by Araxis (commercial, for Windows, Mac OS X and WINE, with trial)
#    - very poor results
#    - even more advanced UI, but even more bad results, sometimes completly useless
#
# Other tools checked, which essentially doesn't support moved blocks detecion:
#   BeyondCompare by Scooter Software, Inc. (Mac OS X, Linux)
#   P4Merge: Visual Merge Tool by Perfore (Windows, Mac OS X, Linux)
#   DeltaWalker by Deltopia (Mac OS X, Windows, Linux)
#   DiffMerge by SourceGear (Windows, Mac OS X, Linux)
#   Code Compare by Devart (Windows)
#   ECMerge by Ellie Computing (Windows,Mac OS X, Linux, Solaris)
#   Compare It! by Grig Software (Windows)
#   Synchronize It! by Grig Software (Windows)
#   ExamDiff Pro by prestoSoft (Windows)
#   Open source: Meld, KDiff3, tkdiff, Advanced Diff (Linux, Windows, Mac OS X)
#
# Pretty sad, so many tools, non does main job good.
#
# I may think about creating GUI ala Meld, WinMerge, KDiff3 or Araxis Merge.
# I will probably use GTK+ (version 3), so stay tuned.

import argparse

parser = argparse.ArgumentParser(description='Displays difference between two files.')
parser.add_argument('files', metavar='file', type=str, nargs=2,
                    help='Files to compare')
parser.add_argument('--fuzzy', dest='fuzzy', action='store_true',
                    default=False,
                    help='Use fuzzy matching, this considers lines similar if only small part of it was changed')
parser.add_argument('--color', dest='color', action='store_true',
                    default=True,
                    help='Use color output')
parser.add_argument('--no-color', dest='color', action='store_false',
                    help='Disable color output')
parser.add_argument('--line-prefixes', dest='line_prefixes', action='store_true',
                    help='Add l and r before line numbers')
parser.add_argument('--summary-left', dest='summary_left', action='store_true',
                    help='Show separetly removed lines from left file')
parser.add_argument('--summary-right', dest='summary_right', action='store_true',
                    help='Show separetly add lines to right file')
parser.add_argument('--summary-stats', dest='summary_stats', action='store_true',
                    default=True,
                    help='Show statistics of diff, after printing diff')
parser.add_argument('--no-summary-stats', dest='summary_stats', action='store_false',
                    default=False,
                    help='Oposite of --summary-stats')
parser.add_argument('--initial-context-matchning', type=int,
                    dest='initial_context_matching',
                    default=3,
                    help='Tweek internal initial context matching length (in general range 3-11 should be sensible)')
parser.add_argument('--fuzzy-threshold', dest='fuzzy_threshold', type=float,
                    default=0.80,
                    help='How similar lines need to be to be considered a match.')
parser.add_argument('--fuzzy-weighted', dest='fuzzy_weighted', action='store_true',
                    default=False,
                    help='When doing fuzzy matching in larger, blocks, use adaptive similarity, depending on how big the block is. I.e. allow larger differences on some lines, as long the whole block has suffiently low differences')
parser.add_argument('--shorten-long-matches', type=int,
                    dest='shorten_long_matches',
                    default=16,
                    help='Matched blocks longer than this will be shortened')
parser.add_argument('--context', type=int,
                    dest='context_length',
                    default=3,
                    help='When shortening long matches, keep this many lines before and after any modification or fuzzy match, or a start and end of a moved block')
parser.add_argument('--compact-delta', dest='commonalize_diff_compact_delta', action='store_true',
                    default=False,
                    help='Ignore whitespaces, when doing similarity checks, i.e. changed indentation, or adding spaces around binary operators to improve code style, etc.. Consider them fully matching basically')
parser.add_argument('--skip-equal', action='store_true',
                    dest='skip_equal',
                    help='If there are no difference at all, or there are big equal blocks at the start or end, skip them. This makes wbdiff be silent on exactly equal files (similar to other diff tools)')
parser.add_argument('--use-default-replacments',
                    dest='use_default_replacments', action='store_true',
                    help='Apply default replacments rules from wbdiff on input files')
parser.add_argument('--read-compressed-files', action='store_true',
                    default=True,
                    dest='read_compressed_files',
                    help='If input files are compressed (like .gz, .bz2), decompress prior to performing actuall diff (enabled by default)')
parser.add_argument('--no-read-compressed-files', action='store_false',
                    dest='read_compressed_files',
                    help='Do not attempt to decompress compressed files')
parser.add_argument('--no-header', action='store_false',
                    dest='header',
                    help='Do not show the explation header at the start')
parser.add_argument('--no-preamble', action='store_false',
                    dest='preamble',
                    help='Do not show the preamble, "--- left", "+++ right" at the start')
parser.add_argument('--no-frames', action='store_false',
                    dest='frames',
                    help='Do not show the frames (i.e. "-----------\ l0202–l0206 -> r0273–r0277  (4 lines)") around each matched block with line range info, ')
parser.add_argument('--git', action='store_true',
                    dest='git',
                    help='Enable git mode. Exit with status 0, even for different files. And print extra two lines at the end of the output.')
parser.add_argument('--debug', action='store_true',
                    dest='debug',
                    help='Enable various debugging reports, and internal consistency checks')


args = parser.parse_args()

#16 # lenght of biggest block to show, above that,
                          # we will split block in half, and show ... (dot dot dot)


# Usage of backreferences in "regular expressions" here,
# is explicitly supported by wbdiff.
# They are not in formal sense a regular expressions,
# but they should be very short, simple, most of them
# should also be fast, and are very usefull here, so
# why not.
# Replacments are applied prior to comparision of lines,
# so they are usefull method of discarding common classes
# of small changes.
# They will still be displayed, but in slightly different way.
replacments = {
  "Incrementation": (r"\b([a-zA-Z_][a-zA-Z0-9_]*)\s*=\1\s*\+\s*1\b", r"\1++"),
  "Decrementation": (r"\b([a-zA-Z_][a-zA-Z0-9_]*)\s*=\1\s*-\s*1\b", r"\1--"),
  "Leading whitespace": (r"^\s+", r""),
  "Trailing whitespace": (r"\s+$", r""),
  "Log timestamp": (r"^(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) [012][0-9] [012][0-9]:[0-6][0-9]:[0-6][0-9] (\w+(?:\.\w+)*) kernel: \[ *[0-9]+\.[0-9]+\] ", r"\1  kernel:  ")
}

# http://stackoverflow.com/questions/287871/print-in-terminal-with-colors-using-python
class _ansi:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = "\033[1m"

class _nocolor:
  HEADER = ""
  OKBLUE = ""
  OKGREEN = ""
  WARNING = ""
  FAIL = ""
  ENDC = ""
  BOLD = ""

ansi = _ansi()
if not args.color:
  ansi = _nocolor

# Handy helpers to color just short string inside longer string.
def blue(s):
  return ansi.OKBLUE + s + ansi.ENDC

def red(s):
  return ansi.FAIL + s + ansi.ENDC

def green(s):
  return ansi.OKGREEN + s + ansi.ENDC

def yellow(s):
  return ansi.WARNING + s + ansi.ENDC

debug = args.debug
commonalize_diff_compact_delta = args.commonalize_diff_compact_delta
fuzzy_adaptive = args.commonalize_diff_compact_delta

# Read files in in full, and split into lines.
leftfilename = args.files[0]
rightfilename = args.files[1]

# We display the header as soon as possible, so in case of crash,
# we it is easier to know which files were involved.
if args.header:
  print(red("Removals") + " from left file, " + blue("moves") + " and " + green("additions") + " to right file:")
if args.preamble:
  print(red("--- " + leftfilename))
  print(green("+++ " + rightfilename))

file_reader1 = lambda filename: open(filename, "r")
file_reader2 = lambda filename: open(filename, "r")
if args.read_compressed_files:
  if leftfilename.endswith(".gz"):
    import gzip
    file_reader1 = gzip.GzipFile
  if rightfilename.endswith(".gz"):
    import gzip
    file_reader2 = gzip.GzipFile
  if leftfilename.endswith(".bz2"):
    import bzip2
    file_reader1 = gzip.BZ2File
  if rightfilename.endswith(".bz2"):
    import bzip2
    file_reader2 = gzip.BZ2File

left = file_reader1(leftfilename).readlines()
right = file_reader2(rightfilename).readlines()
# We do not remove end lines, as this is helpful for the last line diffing.

import difflib

def diff(a, b):
  d = difflib.Differ()
  di = d.compare(a.splitlines(True), b.splitlines(True))
  if commonalize_diff_compact_delta:
    di = filter(lambda x: x[0] != " ", di)
  return "".join(di)

def similarity(a, b):
  # TODO(baryluk): Disable autojunk heuristics above 5000 lines.
  # Then, after crude matching and block reconstruction,
  # recompute it again more accurately.
  sequence_matcher = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)
  # sequence_matcher.quick_ratio()  # TODO(baryluk): Use it maybe for over 5000 lines? It would help a little.
  return sequence_matcher.ratio(), sequence_matcher

fuzzy = args.fuzzy
fuzzy_threshold = args.fuzzy_threshold

def line_similarity(a, b):
  if not fuzzy:
    if a == b:
      return 1.0
    else:
      return 0.0
  # TODO(baryluk): Cache sequencer_matcher?
  ratio, sequencer_matcher = similarity(a, b)
  # TODO(baryluk): Boost a similarity, if substantial prefix or suffix are the same, and there are only inserts or removals.
  # I.e. AAAAAAAAAAAAAAAAAAAA
  #      AAAAAAAAAAAAAAAAAAAA   # KAJSLKJDASLKJDLASKJDLAKSJDLKAJSLKAJSLKDJ
  # They should match as similar.
  return ratio

def similar(a, b, threshold):
  ratio, sequencer_matcher = similarity(a, b)
  return ratio > threshold

import re

def compile_replacments():
  compiled_replacments = {}
  for n, from_to in replacments.iteritems():
    compiled_replacments[n] = re.compile(from_to[0]) #, flags=re.IGNORECASE
  return compiled_replacments

def apply_replacements_line(compiled_replacments, line):
  for n, from_to in replacments.iteritems():
    line = compiled_replacments[n].sub(from_to[1], line)
  return line

def apply_replacements(compiled_replacments, org):
  for i, line in enumerate(org):
    org[i] = apply_replacements_line(compiled_replacments, line)
  return org

if args.use_default_replacments:
  compiled_replacments = compile_replacments()
  left = apply_replacements(compiled_replacments, left)
  right = apply_replacements(compiled_replacments, right)

# In loops below:
#  * i index is used for left file.
#  * j index is used for right file.
#  * k or kk, is used as an offset from current i and j.
#  * a is a left file line connent at index i.
#  * b is a right file line connent at index j.
#
# Remember i, j are zero-based. As is k and kk.
# k, kk - after finishing some loops, might
# be past the end of file. It will indicate a count matching lines
# or equivalently a number of loop iterations performed.



# Compute similarity matrix for each line pair.
# Each entry is between 0.0 and 1.0 inclusive.
leftlen = len(left)
rightlen = len(right)

import sys

# Heuristic for equal files.
if leftlen == rightlen:
  equal = True
  for i in range(leftlen):
    if left[i] != right[i]:
      equal = False
      break
  if equal:
    if args.skip_equal:
      sys.exit(0)


m = list(range(leftlen))
# TODO(baryluk): This doesn't scale above 5000+ lines on each side.
# Combine this with a next loop, that does block merging,
# and do on the fly, this way we can be almost linear, instead
# of quadratic.

for i, a in enumerate(left):
  m[i] = list(range(rightlen))
  for j, b in enumerate(right):
    m[i][j] = line_similarity(a, b)

from math import log, exp

# Use context for similarity measure.
m2 = list(range(leftlen))
for i in range(leftlen):
  m2[i] = list(range(rightlen))
  # TODO(baryluk): Doing a full L*R checks, is a bit wasteful.
  # We can improve it a lot. If we find a good long block,
  # consider it final, and don't try to search it again anywhere.
  #
  # TODO(baryluk): But, what about code duplication. I.e. One big
  # block is copy-pasted several times into new file. Should we detect it
  # maybe warn to use function, macro, template, class, etc?
  for j in range(rightlen):
    matched_line_count = 0
    block_similarity_ratio = 1.0
    log_block_similarity_ratio = 0.0
    # Extend using context, but do no go beyond `context_matching` limit
    # or last lines.
    for k in range(args.initial_context_matching):
      if i + k >= leftlen or j + k >= rightlen:
        # End of file on one side.
        break
      similarity_ratio = m[i + k][j + k]
      if similarity_ratio < fuzzy_threshold:
        break
      matched_line_count += 1
      if fuzzy_adaptive:
        block_similarity_ratio *= similarity_ratio
        log_block_similarity_ratio += log(similarity_ratio)
    if matched_line_count == 0:
      m2[i][j] = (0.0, 0)
      continue
    assert matched_line_count >= 1, f"At least one line need to match or be similar in the block, got {matched_line_count}! i = {i} , j = {j}"
    if fuzzy_adaptive:
      block_similarity_ratio = block_similarity_ratio ** (1.0 / matched_line_count)
      avg_log_block_similarity_ratio = log_block_similarity_ratio / matched_line_count
      block_similarity_ratio = exp(avg_log_block_similarity_ratio)
    if fuzzy_adaptive and False:
      # Some tweeking to weights
      if block_similarity_ratio < fuzzy_threshold:
        # If we are under threshold, then consider them not interesting.
        block_similarity_ratio = 0.0
      else:
        # If above threshold, stretch it and make non-linear.
        # I.e. stretch 0.90 – 1.0, to 0.0 – 1.0
        block_similarity_ratio = (block_similarity_ratio - fuzzy_threshold) * (1.0 - fuzzy_threshold)
        # Apply non-linearity.
        block_similarity_ratio = 1.0 - (1.0 - block_similarity_ratio)**2
    m2[i][j] = (block_similarity_ratio, matched_line_count)

# m - similarity ratios between idividual lines.
# m2 - similarity between extended blocks, anchored at specific starting lines
#      Each extended block is exteneded as much as possible in general, as long
#      there are similarities in adjacent lines.
#      m2 can contain various overlaps between extended blocks, and blocks
#      might be of varying sizes.

# Flag for each line, if it already was matched to something,
# assigned a block. This also speeds up LCS a bit.
matched_left = [False] * leftlen
matched_right = [False] * rightlen

#if debug:
#  print("L:", "".join('T' if x else 'F' for x in matched_left), f"T: {matched_left.count(True)}", f"F: {matched_left.count(False)}", f"Length: {leftlen}")
#  print("R:", "".join('T' if x else 'F' for x in matched_right), f"T: {matched_right.count(True)}", f"F: {matched_right.count(False)}", f"Length: {rightlen}")


# Indexed by first right line number in the block.
# The values are matching starting line on the left side,
# and the length of the block in lines.
#
# Example: block_right[7] = (2, 10), represents block of size 10: l3–l12 -> r8–r17 (inclusive). Not a typo.
blocks_right = {}

def remove_nl(s):
  if s and s[-1] == '\n':
    return s[:-1]
  else:
    return s

def escape_tab(s):
  return s.replace("\t", "\\t")

def pretty(s):
  return escape_tab(s).replace("\n", "\\n")

# Search for long common subsequences (LCS), and extend them,
# forming blocks.
for j in range(rightlen):
  if matched_right[j]:
    continue
  for i in range(leftlen):
    #if matched_right[j]:
    #  break
    if matched_left[i]:
      continue
    block_similarity_ratio, matched_line_count = m2[i][j]
    assert block_similarity_ratio >= 0.0
    assert block_similarity_ratio <= 1.0
    if matched_line_count == 0:
      assert block_similarity_ratio == 0.0
    else:
      assert block_similarity_ratio > 0.0
    if block_similarity_ratio >= fuzzy_threshold:
      assert m[i + 0][j + 0] >= fuzzy_threshold, f"Expected: m[i + 0][j + 0] = m[{i} + 0][{j} + 0] = {m[i + 0][j + 0]} >= {fuzzy_threshold} = fuzzy_threshold"
      #matched_left[i] = True
      #matched_right[j] = True
      k = 0  # Matched lines so far.
      max_width = 0
      while i + k < leftlen and j + k < rightlen and m[i + k][j + k] >= fuzzy_threshold:  # m not m2 !
        if matched_left[i + k]:
          break
        if matched_right[j + k]:
          break
        if matched_left[i + k]:
          _l = left[i + k]
          _r = right[matched_left[i + k] - 1]
          print(f"left[{i} + {k}] = '{pretty(_l)}'")
          print(f"right[matched_left[{i} + {k}] - 1] = right[{matched_left[i + k] - 1}] = '{pretty(_r)}'")
        assert not matched_left[i + k], f"Expected: matched_left[i + k] = matched_left[{i} + {k}] = {matched_left[i + k]} == False; i={i}, j={j}, k={k}"
        assert not matched_right[j + k], f"Expected: matched_right[j + k] = matched_right[{j} + {k}] = {matched_right[j + {k}]} == False; i={i}, j={j}, k={k}"
        matched_left[i + k] = j + k + 1
        matched_right[j + k] = i + k + 1
        max_width = max(max_width, len(left[i + k]))
        k += 1
      if k >= 1 and (k >= 2 or max_width >= 1) and (max_width > 0):  # Don't consider short small addition to be moves.
        # No more similiarity, terminate the block, and add it to the list of blocks.
        assert k >= 1, f"Expected: k = {k} >= 1. Internal logic error"
        assert matched_left[i + 0]
        assert matched_left[i + 0] == j + 1
        assert matched_right[j + 0]
        assert matched_right[j + 0] == i + 1
        blocks_right[j] = (i, k)
        # break  # No point matching more line on left against right[j]

# At this point, we now have a nice nice list of matching blocks.
# All that is left to do is display them.
# Additionally we need to: display removed lines from left
# file at the start, order matching blocks, interleave
# removed and added new lines.
# But all the hard fuzzy logic and block matching is already done above.

# Debug check. Check some invariants, etc.
if debug:
  total_matched = 0
  for j, (i, k) in blocks_right.items():
    assert k > 0, f"Expected: k = {k} > 0"
    assert j + k <= rightlen, f"Expected j + k = {j} + {k} = {j + k} <= {rightlen} = rightlen"
    assert i + k <= leftlen, f"Expected i + k = {i} + {k} = {i + k} <= {leftlen} = leftlen"
    for kk in range(k):
      assert m[i + kk][j + kk] >= fuzzy_threshold, "Expected: m[{i} + {kk}][{i} + {kk}] = {m[i + kk][j + kk]} >= {fuzzy_threshold} to be true"
      assert matched_left[i + kk], "Expected: matched_left[i + kk] = matched_left[{i} + {kk}] = {matched_left[i + kk]} > 0"
      assert matched_right[j + kk], "Expected: matched_right[j + kk] = matched_right[{j} + {kk}] = {matched_right[j + kk]} > 0"
      assert matched_left[i + kk] == j + kk + 1
      assert matched_right[j + kk] == i + kk + 1
      assert line_similarity(left[i + kk], right[j + kk]) >= fuzzy_threshold
      total_matched += 1
  assert total_matched <= leftlen, f"Expected: total_matched = {total_matched} <= {leftlen} = leftlen"
  assert total_matched <= rightlen, f"Expected: total_matched = {total_matched} <= {rightlen} = rightlen"
  #print("Total matched:", total_matched)
  #print("L:", "".join('T' if x else 'F' for x in matched_left), f"T: {matched_left.count(True)}", f"F: {matched_left.count(False)}", f"Length: {leftlen}")
  #print("R:", "".join('T' if x else 'F' for x in matched_right), f"T: {matched_right.count(True)}", f"F: {matched_right.count(False)}", f"Length: {rightlen}")
  assert (leftlen - matched_left.count(False)) == total_matched, f"Expected: matched_left.count(True) = {matched_left.count(True)} == {total_matched} = total_matched"
  assert (rightlen - matched_right.count(False)) == total_matched, f"Expected: matched_right.count(True) = {matched_right.count(True)} == {total_matched} = total_matched"


# Compute proper width of line number column.
W = 2
if max(leftlen, rightlen) >= 100:
  W = 3
if max(leftlen, rightlen) >= 1000:
  W = 4
if max(leftlen, rightlen) >= 10000:
  W = 5
W += 1  # workaround around IMHO % 3d bug, TODO(baryluk): Fix it.

# Line prefixes.
LL = ""  # Left
RR = ""  # Right
EE = ""  # Not-present lines.
if args.line_prefixes:
  LL = "l"
  RR = "r"
  EE = " "

# Formating of the left column, with line numbers, for:
WW = "%s% " + str(W) + "d"  # ?? Some remnant of the past?
WL = LL + "% " + str(W) + "d"  # Left
WR = RR + "% " + str(W) + "d"  # Right
XX = EE + " "*W  # The other side, when only one is present (i.e. removals from left, or additions to right).

# These things usually will be used in form like this:
# WL->WR
# WL->XX
# XX->WR


mark_equal, mark_insert, mark_delete, mark_replace = blue, green, red, (lambda l, r: red(l) + green(r))
if not args.color:
  mark_equal = lambda x: x
  mark_insert = lambda x: f"[+{x}+]"
  mark_delete = lambda x: f"[-{x}-]"
  mark_replace = lambda l, r: mark_delete(l) + mark_insert(r)

# For similar lines, returns a colored or un-colored representation with all the edits on it.
def word_diff(l, r):
  assert fuzzy, "Internal error: word_diff used, but fuzzy is not set"
  if l == r:
    return blue(l)
  if l[-1] == '\n':
    l = l[:-1]
  if r[-1] == '\n':
    r = r[:-1]
  ratio, sequence_matcher = similarity(l, r)
  #if __debug__:
  #  assert ratio >= fuzzy_threshold, f"Expected: ratio = {ratio} >= {fuzzy_threshold}"
  ret = ""
  # TODO(baryluk): I am assuming these are essentially sorted.
  for opcode_tag, i1, i2, j1, j2 in sequence_matcher.get_opcodes():
    if opcode_tag[0] == "e":  # "equal"
      # assert l[i1:i2] == r[j1:j2]
      ret += mark_equal(l[i1:i2])
    elif opcode_tag[0] == "i":  # "insert"
      ret += mark_insert(r[j1:j2])
    elif opcode_tag[0] == "d":  # "delete"
      ret += mark_delete(l[i1:i2])
    elif opcode_tag[0] == "r":  # "replace"
      ret += mark_replace(l[i1:i2], r[j1:j2])
  if debug:
    return ret + f" (similarity ratio: {ratio})"
  return ret


# Print common subsequences, intermixing new lines (in natural order),
# and removed ones (just after end of moved ones).

# Keep track of stats for the use in the summary at the end.
lines_moved = 0
lines_not_moved = 0
left_right_matched = 0
left_removed = 0
right_added = 0
files_differ = False

# Various '+ 1', are because for human usage, we want lines to start from 1,
# but internally we keep lines in lists, which start indexing at 0.
# So for display we need to do '+ 1', but for accessing the lists itself,
# we do not!

# First print lines removed from left file, before any blocks in right file.
for i, a in enumerate(left):
  if not matched_left[i]:
    left_removed += 1
    print((WL + "->" + XX + " " + red("- %s")) % (i + 1, a.rstrip()))
  else:
    break


# Right centric view.
for j, _b in enumerate(right):
  if j in blocks_right:
    i, k = blocks_right[j]
    assert k >= 1
    if i != j:
      files_differ = True
    if args.frames:
      print("-"*(2 * W + 3) + "\\ l%04d–l%04d -> r%04d–r%04d  (%d lines)" % (i + 1, i + k + 1, j + 1, j + k + 1, k))
    consequtive_equals = 0
    kk = 0
    while kk < k:
      similarity_ratio = line_similarity(left[i + kk], right[j + kk])
      if fuzzy:
        assert similarity_ratio >= fuzzy_threshold, f"Expected: left[{i} + {kk}] ~ right[{j} + {kk}], got {pretty(left[i + kk])} != {pretty(right[j + kk])}, Expected: similarity_ratio = {similarity_ratio} >= {fuzzy_threshold}"
      else:
        assert left[i + kk] == right[j + kk], f"Expected: left[{i} + {kk}] == right[{j} + {kk}], got {pretty(left[i + kk])} != {pretty(right[j + kk])}"
        if debug: assert similarity_ratio == 1.0, f"Expected: left[{i} + {kk}] ~== right[{j} + {kk}], got {pretty(left[i + kk])} == {pretty(right[j + kk])}, but similarity_ratio = {similarity_ratio} != 1.0"
      if consequtive_equals == 0:
        # Count ahead exact equals for fold equal logic.
        # The result can still be zero.
        while kk + consequtive_equals < k and left[i + kk + consequtive_equals] == right[j + kk + consequtive_equals]:
          consequtive_equals += 1
      assert kk + consequtive_equals <= k, f"Failed: {kk} + {consequtive_equals} <= {k}"
      if consequtive_equals >= args.shorten_long_matches:
        left_right_matched += consequtive_equals
        if i == j:
          lines_not_moved += consequtive_equals
        else:
          lines_moved += consequtive_equals
        # Due to fuzzy matching, we might have long matching block,
        # but it doesn't mean all lines are exactly equal.
        assert left[i + kk] == right[j + kk]
        print((WL + "->" + WR + " " + blue("| %s")) % (i + kk + 1, j + kk + 1, remove_nl(left[i + kk])))
        kk += 1
        skipped_count = consequtive_equals - 2
        assert skipped_count >= 5
        #assert k - args.shorten_long_matches + 3 > 3, f"It is silly to shorten {k - args.shorten_long_matches + 1} lines, to... 3 lines."
        print(" " + "."*(W - 1) + "->" + " " + "."*(W - 1) + " " + blue(":"))
        print(f" ({skipped_count} lines skipped)")
        print(" " + "."*(W - 1) + "->" + " " + "."*(W - 1) + " " + blue(":"))
        kk += skipped_count
        assert kk < k
        assert left[i + kk] == right[j + kk]
        print((WL + "->" + WR + " " + blue("| %s")) % (i + kk + 1, j + kk + 1, remove_nl(left[i + kk])))
        kk += 1
        consequtive_equals = 0
      else:
        left_right_matched += 1
        if i != j:
           lines_moved += 1
        else:
           lines_not_moved += 1
        if fuzzy and left[i + kk] != right[j + kk]:
          assert consequtive_equals == 0
          consequtive_equals = 0
          files_differ = True
          print((WL + "->" + WR + " " + yellow("M") + " %s") % (i + kk + 1, j + kk + 1, remove_nl(word_diff(left[i + kk], right[j + kk]))))
        else:
          assert left[i + kk] == right[j + kk]
          print((WL + "->" + WR + " " + blue("| %s")) % (i + kk + 1, j + kk + 1, remove_nl(left[i + kk])))
          consequtive_equals -= 1
          assert consequtive_equals >= 0
        kk += 1
    if args.frames:
      print("-"*(2 * W + 3) + "/")
    i = i + k
    while i < leftlen and not matched_left[i]:
      left_removed += 1
      files_differ = True
      print((WL + "->" + XX + " " + red("- %s")) % (i + 1, left[i].rstrip()))
      i += 1
  if not matched_right[j]:
    assert not j in blocks_right
    right_added += 1
    files_differ = True
    print((XX + "->" + WR + " " + green("+ %s")) % (j + 1, _b.rstrip()))

if args.summary_left:
  print(red("Removed") + " from left file:")
  for i, a in enumerate(left):
    if not matched_left[i]:
      if i > 0 and matched_left[i - 1]:
        if args.frames:
          print("-----")
      print((WL + "->" + XX + " " + red("- %s")) % (i + 1, a.rstrip()))

if args.summary_right:
  print(green("Added") + " to right file:")
  for j, b in enumerate(right):
    if not matched_right[j]:
      if i > 0 and matched_right[j - 1]:
        if args.frames:
          print("-----")
      print((XX + "->" + WR + " " + green("+ %s")) % (j + 1, b.rstrip()))

if args.summary_stats:
  print()
  print(ansi.WARNING + "Diff statistics:")
  print(f"\tInitial context length used: {args.initial_context_matching}")
  print(f"\tNumber of matching (extended) blocks: {len(blocks_right)}")
  # sum(map(lambda x: x[1], blocks_right.itervalues()))
  print(f"\tNumber of lines in matching (extended) blocks: {left_right_matched}")
  print(f"\tNumber of lines removed from left file: {left_removed}")
  print(f"\tNumber of new lines added to right file: {right_added}")
  if debug:
    print(f"\tNumber of lines moved (exact and fuzzy): {lines_moved}")
    print(f"\tNumber of lines not moved (exact and fuzzy): {lines_not_moved}")
  print(f"\tWeighted cost (blocks + removed + added): {len(blocks_right) - 1 + left_removed + right_added}" + ansi.ENDC)

if files_differ:
  if args.git:
    if args.header:
      print()  # Separate more, if there is a header.
    print()  # Extra line, to separate multiple diffs for separate file pairs.
    sys.exit(0)
  sys.exit(1)

if args.skip_equal:
  assert False, "Should not reach this point"

sys.exit(0)
